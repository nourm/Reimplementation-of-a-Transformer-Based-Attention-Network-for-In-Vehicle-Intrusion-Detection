{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwotc0vAf2wM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from statistics import mode\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paNm1Ajxj9cd"
      },
      "source": [
        "### **Model Construction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNr9Dqm5gRYR"
      },
      "outputs": [],
      "source": [
        "# Positional encoding\n",
        "\n",
        "#Here we will preform positional encoding using Pytorch\n",
        "class PositionalEncoding(nn.Module):\n",
        "   # embedding_dim: dimensionality of the embeddings and max_seq_length: maximum sequence length ( in our case number of features)\n",
        "   # dropout: for each training iteration, approximately 10% of the neurons in the dropout layer will be randomly set to zero.\n",
        "   # register_buffer: used to register a tensor as a buffer in the model.\n",
        "\n",
        "    def __init__(self, embedding_dim, max_seq_length=10):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.register_buffer('positional_encoding', self.calculate_positional_encoding())\n",
        "\n",
        "    # This method computes the positional encoding matrix.\n",
        "\n",
        "    def calculate_positional_encoding(self):\n",
        "\n",
        "        # unsqueeze function is used to add a dimension with size 1.\n",
        "        # position is a tensor representing the position of each element in the sequence.\n",
        "\n",
        "        position = torch.arange(0, self.max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2).float() * (-math.log(10000.0) / self.embedding_dim))\n",
        "        positional_encoding = torch.zeros(self.max_seq_length, self.embedding_dim)\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return positional_encoding.unsqueeze(0)\n",
        "\n",
        "    # This method adds the precomputed positional encoding to the input tensor, applies dropout to the modified tensor and returns the result.\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_encoding[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Define the encoder layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    # num_heads: The number of heads in the multihead attention mechanism.\n",
        "    # ff_dim: The dimensionality of the feedforward layer's intermediate representation.\n",
        "    # dropout: The dropout rate, with a default value of 0.1.\n",
        "    def __init__(self, embedding_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.multihead_attention = nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
        "        self.feedforward = nn.Sequential(\n",
        "\n",
        "            # The first linear transformation in the feedforward layer takes the input tensor with embedding_dim dimensions\n",
        "            # and transforms it into an intermediate representation with ff_dim dimensions.\n",
        "            nn.Linear(embedding_dim, ff_dim),\n",
        "            # ReLU introduces non-linearity by replacing negative values with zero.\n",
        "            nn.ReLU(),\n",
        "            #The second linear transformation takes the representation with ff_dim dimensions\n",
        "            # and transforms it back to the original embedding dimensionality.\n",
        "            nn.Linear(ff_dim, embedding_dim)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # retrieve attention output from the multihead_attention\n",
        "        # x, x, x represent : Q, K, V\n",
        "        attn_output, _ = self.multihead_attention(x, x, x)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        # Layer normalization is applied to the sum of the input tensor and the attention output.\n",
        "        x = self.layer_norm1(x)\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.layer_norm2(x)\n",
        "        return x\n",
        "\n",
        "# Define the transformer encoder stack\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    # num_layers: The number of transformer encoder layers in the stack.\n",
        "    # embedding_dim: The dimensionality of the input and output embeddings.\n",
        "    # num_heads: The number of heads in the multihead attention mechanism.\n",
        "    # ff_dim: The dimensionality of the feedforward layer's intermediate representation.\n",
        "    # max_seq_length: The maximum sequence length (used for positional encoding).\n",
        "    # dropout: The dropout rate, with a default value of 0.1.\n",
        "\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, ff_dim, max_seq_length, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embedding_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim, max_seq_length)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Define the max-pooling layer\n",
        "class MaxPoolingLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Apply max-pooling along the sequence dimension (1D for sequential data)\n",
        "        pooled_output, _ = x.max(dim=1)\n",
        "        return pooled_output\n",
        "\n",
        "# Define the Dense layer\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.dense = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense(x)\n",
        "\n",
        "# Define the classifier layer\n",
        "class ClassifierLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ClassifierLayer, self).__init__()\n",
        "        self.classifier = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Build the complete model\n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_layers, num_heads, ff_dim, max_seq_length, dropout=0.1):\n",
        "        super(CustomTransformerModel, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.transformer_encoder = TransformerEncoder(num_layers, embedding_dim, num_heads, ff_dim, max_seq_length, dropout)\n",
        "        self.max_pooling_layer = MaxPoolingLayer()\n",
        "        self.dense_layer = DenseLayer(embedding_dim, 32)\n",
        "        self.classifier_layer = ClassifierLayer(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        # Transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Max-pooling\n",
        "        x = self.max_pooling_layer(x)\n",
        "\n",
        "        # Dense layer\n",
        "        x = self.dense_layer(x)\n",
        "\n",
        "        # Classifier layer\n",
        "        x = self.classifier_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "vocab_size = 65535  # largest size of data (data is in Hexadecimal => largest possible value \"FFFF\" = 65535)\n",
        "embedding_dim = 32  # embedding dimension\n",
        "num_layers = 6  # Number of transformer layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "ff_dim = 32  # Feedforward dimension\n",
        "max_seq_length = 12  # number of features\n",
        "\n",
        "model = CustomTransformerModel(vocab_size, embedding_dim, num_layers, num_heads, ff_dim, max_seq_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8w6ltULkE5u"
      },
      "source": [
        "### **Data Preprocessing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6Ou8_7ongKJI",
        "outputId": "32ee0215-e798-4ae0-d181-ab6fe044af8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CAN ID  DLC  DATA0  DATA1  DATA2  DATA3  DATA4  DATA5  DATA6  DATA7  \\\n",
              "0         1349    8    216      0      1    138      2      3      4      5   \n",
              "1          688    5    255    127      0      5     73    256    256    256   \n",
              "2            2    8      0      0      0      0      0      1      7     21   \n",
              "3          339    8      0     33     16    255      0    255      0      0   \n",
              "4          304    8     25    128      0    255    254    127      7     96   \n",
              "...        ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "199666     848    8      5     32    244    104    119      0      0    206   \n",
              "199667    1072    8      0      0      0      0      0      0      0      0   \n",
              "199668    1201    8     41     39     39     35      0      0      0    154   \n",
              "199669     497    8      8      0      0      0      0      0      0      0   \n",
              "199670     339    8      0     33     16    255      0    255      0      0   \n",
              "\n",
              "        Flag  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "...      ...  \n",
              "199666     0  \n",
              "199667     0  \n",
              "199668     0  \n",
              "199669     0  \n",
              "199670     0  \n",
              "\n",
              "[199671 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe5eb39d-c16b-406b-851b-678c96f26d7b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CAN ID</th>\n",
              "      <th>DLC</th>\n",
              "      <th>DATA0</th>\n",
              "      <th>DATA1</th>\n",
              "      <th>DATA2</th>\n",
              "      <th>DATA3</th>\n",
              "      <th>DATA4</th>\n",
              "      <th>DATA5</th>\n",
              "      <th>DATA6</th>\n",
              "      <th>DATA7</th>\n",
              "      <th>Flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1349</td>\n",
              "      <td>8</td>\n",
              "      <td>216</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>138</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>688</td>\n",
              "      <td>5</td>\n",
              "      <td>255</td>\n",
              "      <td>127</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>73</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>339</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>16</td>\n",
              "      <td>255</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>304</td>\n",
              "      <td>8</td>\n",
              "      <td>25</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>254</td>\n",
              "      <td>127</td>\n",
              "      <td>7</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199666</th>\n",
              "      <td>848</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>244</td>\n",
              "      <td>104</td>\n",
              "      <td>119</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>206</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199667</th>\n",
              "      <td>1072</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199668</th>\n",
              "      <td>1201</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>154</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199669</th>\n",
              "      <td>497</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199670</th>\n",
              "      <td>339</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>16</td>\n",
              "      <td>255</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>199671 rows Ã— 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe5eb39d-c16b-406b-851b-678c96f26d7b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe5eb39d-c16b-406b-851b-678c96f26d7b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe5eb39d-c16b-406b-851b-678c96f26d7b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6a66c0b3-9319-4b23-9fc5-65dfb2f9a1cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a66c0b3-9319-4b23-9fc5-65dfb2f9a1cd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6a66c0b3-9319-4b23-9fc5-65dfb2f9a1cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def process_dataframe(df):\n",
        "    # Identify and swap 'Flag' and 'DATA' values\n",
        "    for index, row in df.iterrows():\n",
        "        for col in ['DATA0', 'DATA1', 'DATA2', 'DATA3', 'DATA4', 'DATA5', 'DATA6', 'DATA7']:\n",
        "            value = row[col]\n",
        "            if value == 'R' or value == 'T':\n",
        "                df.at[index, 'Flag'] = value\n",
        "                df.at[index, col] = np.nan\n",
        "\n",
        "    # Check for duplicate rows\n",
        "    #df = df.drop_duplicates()\n",
        "\n",
        "    # Convert hexadecimal values to integers for the specified columns\n",
        "    hex_columns = ['CAN ID', 'DATA0', 'DATA1', 'DATA2', 'DATA3', 'DATA4', 'DATA5', 'DATA6', 'DATA7']\n",
        "    for col in hex_columns:\n",
        "        df[col] = df[col].apply(lambda x: int(str(x), 16) if not pd.isna(x) and isinstance(x, str) else x)\n",
        "\n",
        "    # Check for NaN values in data columns and replace with 256 if NaN exists\n",
        "    data_columns = ['DATA0', 'DATA1', 'DATA2', 'DATA3', 'DATA4', 'DATA5', 'DATA6', 'DATA7']\n",
        "    df[data_columns] = df[data_columns].fillna(int(256))\n",
        "\n",
        "    columns_to_convert = ['DATA2', 'DATA3', 'DATA4', 'DATA5', 'DATA6', 'DATA7']\n",
        "    for column in columns_to_convert:\n",
        "        df[column] = df[column].astype('Int64')\n",
        "\n",
        "\n",
        "    # Encode 'Flag' using LabelEncoder\n",
        "    encoder = LabelEncoder()\n",
        "    df['Flag'] = encoder.fit_transform(df['Flag'])\n",
        "    df = df.drop(columns=['Timestamp'])\n",
        "    #scaler = StandardScaler()\n",
        "    #df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])\n",
        "    return df\n",
        "\n",
        "new_data = [[1478195721.903877, '0545', 8, 'd8', '00', '001', '8a', '002', '003', '004', '005', 'R']]\n",
        "df1= pd.read_csv('Fuzzy_dataset.csv')\n",
        "df1= pd.concat([pd.DataFrame(new_data, columns=df1.columns), df1], ignore_index=True)\n",
        "df1.rename(columns={\n",
        "    '1478195721.903877': 'Timestamp',\n",
        "    '0545': 'CAN ID',\n",
        "    '8': 'DLC',\n",
        "    'd8': 'DATA0',\n",
        "    '00': 'DATA1',\n",
        "    '00.1': 'DATA2',\n",
        "    '8a': 'DATA3',\n",
        "    '00.2': 'DATA4',\n",
        "    '00.3': 'DATA5',\n",
        "    '00.4': 'DATA6',\n",
        "    '00.5': 'DATA7',\n",
        "    'R': 'Flag'\n",
        "}, inplace=True)\n",
        " # Subset the DataFrame to keep the first 0.5% of rows\n",
        "num_rows_to_keep = int(0.7 * len(df1))\n",
        "df1 = df1.iloc[:num_rows_to_keep]\n",
        "source = process_dataframe(df1)\n",
        "source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvVhOOcU8LZi",
        "outputId": "3a3ac0c0-7863-40f3-eb7d-55844fef1af3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    170343\n",
              "1     29328\n",
              "Name: Flag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "source['Flag'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3aX_XEQjuiG"
      },
      "source": [
        "### **Single Message Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P20VGIYhgPF2",
        "outputId": "b96a833b-7fc8-4180-ea4f-6eaef4771fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([139769, 10])\n",
            "tensor([1764,    8,  188,  207,   98,   36,  146,  181,   86,  131])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0dfcba027246>:6: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X = X.astype(np.long)\n",
            "<ipython-input-10-0dfcba027246>:7: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = y.astype(np.long)\n"
          ]
        }
      ],
      "source": [
        "X = source.iloc[:, :-1].values\n",
        "y = source['Flag'].values\n",
        "\n",
        "# Convert data to long type\n",
        "X = X.astype(np.long)\n",
        "y = y.astype(np.long)\n",
        "\n",
        "train_features, test_features, train_target, test_target = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "train_features_tensor = torch.tensor(train_features)\n",
        "train_target_tensor = torch.tensor(train_target)\n",
        "test_features_tensor = torch.tensor(test_features)\n",
        "test_target_tensor = torch.tensor(test_target)\n",
        "\n",
        "train_dataset = TensorDataset(train_features_tensor, train_target_tensor)\n",
        "test_dataset = TensorDataset(test_features_tensor, test_target_tensor)\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(train_features_tensor.size())\n",
        "print(train_features_tensor[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4c6N86VgYv3",
        "outputId": "50b493a5-a76c-4901-c903-593c78716409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.0184\n",
            "Epoch 2/20, Loss: 0.0026\n",
            "Epoch 3/20, Loss: 0.0021\n",
            "Epoch 4/20, Loss: 0.0009\n",
            "Epoch 5/20, Loss: 0.0009\n",
            "Epoch 6/20, Loss: 0.0012\n",
            "Epoch 7/20, Loss: 0.0007\n",
            "Epoch 8/20, Loss: 0.0009\n",
            "Epoch 9/20, Loss: 0.0006\n",
            "Epoch 10/20, Loss: 0.0009\n",
            "Epoch 11/20, Loss: 0.0007\n",
            "Epoch 12/20, Loss: 0.0007\n",
            "Epoch 13/20, Loss: 0.0005\n",
            "Epoch 14/20, Loss: 0.0008\n",
            "Epoch 15/20, Loss: 0.0006\n",
            "Epoch 16/20, Loss: 0.0006\n",
            "Epoch 17/20, Loss: 0.0006\n",
            "Epoch 18/20, Loss: 0.0007\n",
            "Epoch 19/20, Loss: 0.0004\n",
            "Epoch 20/20, Loss: 0.0002\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function and optimizer\n",
        "# Binary cross-entropy loss to measure the dissimilarity between predicted probabilities and true binary labels.\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    total_loss = 0.0  # Initialize the total loss for the epoch\n",
        "\n",
        "    for batch in train_data_loader:\n",
        "        input_data, labels = batch\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_data)\n",
        "\n",
        "        labels = labels.unsqueeze(1)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(logits, labels.float())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss for the epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate and print the average loss for the epoch\n",
        "    average_loss = total_loss / len(train_data_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spFUN5VRiTly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4417bf-67a4-44d6-ba5e-67db13683d77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomTransformerModel(\n",
              "  (embedding_layer): Embedding(65535, 32)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (multihead_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (feedforward): Sequential(\n",
              "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (positional_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (max_pooling_layer): MaxPoolingLayer()\n",
              "  (dense_layer): DenseLayer(\n",
              "    (dense): Linear(in_features=32, out_features=32, bias=True)\n",
              "  )\n",
              "  (classifier_layer): ClassifierLayer(\n",
              "    (classifier): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J4Wu6l5ibGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "298c128a-df2d-43c9-ad14-dda95050e9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     50988\n",
            "           1       1.00      1.00      1.00      8914\n",
            "\n",
            "    accuracy                           1.00     59902\n",
            "   macro avg       1.00      1.00      1.00     59902\n",
            "weighted avg       1.00      1.00      1.00     59902\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient tracking for evaluation\n",
        "    for batch in test_data_loader:\n",
        "        input_data, labels = batch\n",
        "        logits = model(input_data)\n",
        "\n",
        "        # Convert logits to predicted labels\n",
        "        predicted_batch = (logits > 0).long()\n",
        "\n",
        "        # Append true and predicted labels to the lists\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predicted_labels.extend(predicted_batch.cpu().numpy())\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Generate a classification report to get precision, recall, F1-score, etc.\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal', 'Injection'], yticklabels=['Normal', 'Injection'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfa1BrJGkTsZ"
      },
      "source": [
        "### **CAN IDs Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozft8t8AjrHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45017177-fadb-43c6-db1b-dac9bc5479ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (160, 0),\n",
              " (161, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1680, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (160, 0),\n",
              " (161, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1680, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (160, 0),\n",
              " (161, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1680, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (160, 0),\n",
              " (161, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1680, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (704, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (160, 0),\n",
              " (161, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1680, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1440, 0),\n",
              " (1442, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (848, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (790, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (1520, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " (339, 0),\n",
              " (2, 0),\n",
              " (848, 0),\n",
              " (704, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (1088, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (2, 0),\n",
              " (339, 0),\n",
              " (304, 0),\n",
              " (305, 0),\n",
              " (320, 0),\n",
              " (704, 0),\n",
              " (848, 0),\n",
              " (880, 0),\n",
              " (1087, 0),\n",
              " (1088, 0),\n",
              " (1264, 0),\n",
              " (790, 0),\n",
              " (399, 0),\n",
              " (608, 0),\n",
              " (672, 0),\n",
              " (809, 0),\n",
              " (1349, 0),\n",
              " (688, 0),\n",
              " (1072, 0),\n",
              " (1201, 0),\n",
              " (497, 0),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "selected_columns = source[['CAN ID','Flag']]\n",
        "\n",
        "# Convert the selected columns to a NumPy array\n",
        "result = selected_columns.to_numpy()\n",
        "\n",
        "# If you want the result in the specified format, convert it accordingly\n",
        "formatted_result = [(x, y) for x, y in result]\n",
        "\n",
        "window_size = 128\n",
        "batch_size = len(formatted_result)\n",
        "Labels = []\n",
        "Y = [0] * len(formatted_result)\n",
        "Seq = []\n",
        "\n",
        "# Loop over the range to create sequences Xi\n",
        "for i in range(0, batch_size - window_size + 1):\n",
        "    Xi = [formatted_result[i + j][0] for j in range(window_size)]\n",
        "    Seq.append(Xi)\n",
        "\n",
        "# Calculate Y values using the recursive sum\n",
        "for i in range(len(formatted_result)):\n",
        "    Y[i] = sum(formatted_result[j][1] for j in range(max(i - window_size + 1, 0), i + 1))\n",
        "\n",
        "# Loop through the batch\n",
        "for j in range(0, batch_size - window_size):\n",
        "    recur_sum_Yi = sum(Y[i] for i in range(j, j + window_size))\n",
        "    Xj = formatted_result[j][0]\n",
        "\n",
        "    if recur_sum_Yi == 0:\n",
        "        Yj = 0\n",
        "        Labels.append((Xj, Yj))\n",
        "    else:\n",
        "        for i in range(j, j + window_size):\n",
        "            Yi = Y[i]\n",
        "            if Yi != 0:\n",
        "                Yj = Yi\n",
        "                Labels.append((Xj, Yj))\n",
        "\n",
        "count_0 = 0\n",
        "count_1 = 0\n",
        "for _, Yj in Labels:\n",
        "    if Yj == 0:\n",
        "        count_0 += 1\n",
        "    elif Yj == 1:\n",
        "        count_1 += 1\n",
        "\n",
        "Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "ISR5kTpJB6mA",
        "outputId": "9ea493d4-5f8f-49cd-f6db-4bf86ce59dd4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6ab7eecc5fa6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTransformerModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Define loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomTransformerModel' is not defined"
          ]
        }
      ],
      "source": [
        "# Create an instance of the model\n",
        "vocab_size = 65535  # Adjust as needed\n",
        "embedding_dim = 32  # Adjust as needed\n",
        "num_layers = 6  # Number of transformer layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "ff_dim = 32  # Feedforward dimension\n",
        "max_seq_length = 1  # Adjust as needed\n",
        "\n",
        "model = CustomTransformerModel(vocab_size, embedding_dim, num_layers, num_heads, ff_dim, max_seq_length)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Convert your sequence data to PyTorch tensors (assuming `Seq` is your input sequence data and `Labels` is your target labels)\n",
        "X_train = torch.tensor(Seq, dtype=torch.long)  # Assuming Seq contains integer tokens\n",
        "Y_train = torch.tensor([y for _, y in Labels], dtype=torch.long)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    output = model(X_train)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(output.squeeze(), Y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss for monitoring\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9UGmdL1nF1v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}